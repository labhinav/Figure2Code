{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune GIT on a custom dataset for image captioning\n",
        "\n",
        "In this notebook, we'll fine-tune [GIT](https://huggingface.co/docs/transformers/main/en/model_doc/git), short for GenerativeImage2Text, on a toy image captioning dataset.\n",
        "\n",
        "GIT is, at the moment of writing, a state-of-the-art image/video captioning and question answering (QA) model.\n",
        "\n",
        "## Set-up environment\n",
        "\n",
        "First, let's install ü§ó Transformers as well as ü§ó Datasets."
      ],
      "metadata": {
        "id": "4KNaYcxCqn8t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqrW65lfqhhJ",
        "outputId": "78f576d9-7182-4a59-869f-5d022d329ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "lTI8wKxgql9i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install codebleu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faTZc78j4UBe",
        "outputId": "02e8de4a-f192-41af-e3dd-3b29218fcac6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: codebleu in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: tree-sitter<0.23.0,>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from codebleu) (0.22.3)\n",
            "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.10/dist-packages (from codebleu) (67.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tree-sitter-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CQRiA5_8cAB",
        "outputId": "2d13a002-64e7-4ec6-df30-ee9d4247cb3c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tree-sitter-python\n",
            "  Downloading tree_sitter_python-0.21.0-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/130.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m30.7/130.6 kB\u001b[0m \u001b[31m790.0 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.9/130.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tree-sitter-python\n",
            "Successfully installed tree-sitter-python-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we use the [ImageFolder](https://huggingface.co/docs/datasets/main/en/image_dataset#imagefolder) feature to quickly turn this into a ü§ó Dataset. We'll specify that this is just the training split of the dataset."
      ],
      "metadata": {
        "id": "kSagX7N1Mr0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('abhinavl/figure2code_data_square')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bon_IYbzq0vz",
        "outputId": "05ece943-e24b-4fb7-a2df-46cfb05123a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check whether the dataset is created correctly:"
      ],
      "metadata": {
        "id": "iBko9oMhNPNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFy-TnHKNRXV",
        "outputId": "c5eb0d78-0be4-45b9-fe03-7bd7c308ce4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['image', 'labels', 'values', 'title', 'value_heading', 'code', 'og_file_name'],\n",
              "        num_rows: 8000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['image', 'labels', 'values', 'title', 'value_heading', 'code', 'og_file_name'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['image', 'labels', 'values', 'title', 'value_heading', 'code', 'og_file_name'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = dataset['train']\n",
        "dataset_val = dataset['validation']\n",
        "dataset_test = dataset['test']"
      ],
      "metadata": {
        "id": "GhcrNmvdOBLd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at one example:"
      ],
      "metadata": {
        "id": "HCq63T_bM3mK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = dataset_train[0]\n",
        "image = example[\"image\"]\n",
        "width, height = image.size\n",
        "print(width, height)\n",
        "display(image.resize((int(width), int(height))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "dIMVRRn7tOSf",
        "outputId": "9b459f80-fefd-4be9-d9fc-d82d5ea559db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "800 800\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=800x800>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyAAAAMgCAIAAABUEpE/AAA4/UlEQVR4nO3de3xU9Z34/zOAJFkkQQQhICKygFwUIsglFLcuN6FacdvCtgrVopZKFcqD2kZrW3QrxSpyEUT6ULO0FcEGFVtYQeUqWS2Y0Me2aMGipJCsiwUCqFzn98d8nV8KaC18yDDJ8/nXmXM+Z/I+PB4xL89MJrF4PB4BABBOnVQPAABQ0wgsAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwALOOLFPtXLlyhtvvPHCCy9Mrr///vufe+65qs+wcuXKxMrqHRzg/6mX6gEAjlVcXJzcvu+++1asWPHKK68k93Tq1KlVq1bjxo1L7rn//vu//OUvDxs2rDqHBPgUAgs44/Tu3Tu53bRp0zp16lTdE0VRdnZ2tQ8F8A/wEiGQfqq+RBiLxfbv3/+f//mfiRcQP//5z5/wlPXr13/xi19s3LhxZmZmXl7ewoULq21aoBYSWEB6Ky4uzsrKGjp0aHFxcXFx8ezZs49fs2LFir59++7evXvOnDnPP/98t27dRowYUVhYWO3DArWFlwiB9Na7d+86deo0bdr0mJcRq7rttts6d+78yiuv1KtXL4qiwYMH79y586677ho1alSdOv4/EwjPf1mAGm7Lli1vvvnm9ddfH0XR4Y8NHTq0vLz8rbfeSvV0QM3kDhZQw/3v//5vFEUTJ06cOHHiMYd27tyZiomAmk9gATVckyZNoigqKCj4t3/7t2MOdejQIRUTATWfwALSXkZGxocffvhJRzt06NCuXbuNGzfef//91TkVUJsJLCDtXXLJJStXrnzhhRdyc3MbNmx4/H2pxx57bMiQIYMHD77xxhtbtmz517/+ddOmTW+88cYzzzyTkoGBGs+b3IG0N3369Hbt2v37v//75Zdf/s1vfvP4BVdeeeXrr7/eqFGj8ePHDxgw4Fvf+tZLL700YMCA6h8VqCVi8Xg81TMAANQo7mABAAQmsAAAAhNYAACBCSwAgMAEFgBAYAILACAwHzT6WR09enTHjh0NGzaMxWKpngUA0kk8Ht+7d2+LFi3q1Kktd3YE1me1Y8eOVq1apXoKAEhXZWVl559/fqqnqCYC67Nq2LBhFEVlZWXZ2dmpngUA0kllZWWrVq0SP0lrCYH1WSVeGczOzhZYAHASatV7bGrLS6EAANVGYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACS8vAmjx58uWXX96wYcPzzjtv2LBhb7311ietXLVqVffu3TMzMy+66KI5c+ZUPVRUVNSpU6eMjIxOnTo9++yzp39qAKC2SMvAWrVq1dixY//7v/97+fLlhw8fHjRo0P79+49ftnXr1qFDh/br16+kpOSuu+664447ioqKEoeKi4tHjBgxcuTIjRs3jhw5cvjw4a+99lr1XgQAUGPF4vF4qmc4Jf/3f/933nnnrVq16oorrjjm0Pe+973Fixdv2rQp8XDMmDEbN24sLi6OomjEiBGVlZVLly5NHLrqqqvOOeec+fPnf8oXqqyszMnJ2bNnT3Z29mm4DgCosWrhz9C0vINV1Z49e6Ioaty48fGHiouLBw0alHw4ePDg9evXHzp06ISH1q1bd/wzHDhwoLKK8NMDADVRvVQPcEri8fiECRM+97nPdenS5fijFRUVzZo1Sz5s1qzZ4cOHd+7cmZube/yhioqK459h8uTJkyZNOh2TA+nlpyU7Uz0CVJ/v5zVJ9QhpL73vYH3729/+/e9//ykv7cViseR24sXQ5J5jDlV9mFRQULDnY2VlZcHmBgBqtDS+g3X77bcvXrx49erV559//gkXNG/evOp9qffee69evXrnnnvuCQ9VvaGVlJGRkZGREXpwAKCGS8s7WPF4/Nvf/vaiRYteeeWVNm3afNKyPn36LF++PPlw2bJlPXr0OOuss054KD8//7TODADUHmkZWGPHjv3lL3/51FNPNWzYsKKioqKi4sMPP0wcKigoGDVqVGJ7zJgx77777oQJEzZt2vTEE088/vjjEydOTBwaN27csmXLpkyZ8uabb06ZMuWll14aP358Sq4FAKh50jKwHn300T179nz+85/P/diCBQsSh8rLy7dt25bYbtOmzZIlS1auXNmtW7f77rtvxowZX/rSlxKH8vPzn3766SeffPLSSy8tLCxcsGBBr169UnMxAECNk/afg1VtauFneABJfouQWiX4bxHWwp+haXkHCwDgTCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIGla2CtXr36mmuuadGiRSwWe+6550645sYbb4z9rc6dOycOFRYWHnPoo48+qr7pAYAaLV0Da//+/V27dn3kkUc+Zc306dPLP1ZWVta4ceOvfOUryaPZ2dnlVWRmZp7+qQGAWqFeqgc4SUOGDBkyZMinr8nJycnJyUlsP/fcc7t27brpppuSR2OxWPPmzU/jiABAbZWud7D+UY8//viAAQNat26d3LNv377WrVuff/75V199dUlJyQnPOnDgQGUV1TUsAJDeakVglZeXL1269Oabb07uufjiiwsLCxcvXjx//vzMzMy+fftu3rz5+BMnT56c87FWrVpV48gAQBqrFYFVWFjYqFGjYcOGJff07t37hhtu6Nq1a79+/RYuXNi+ffuZM2cef2JBQcGej5WVlVXfxABAOkvX92B9dvF4/Iknnhg5cmT9+vVPuKBOnTqXX375Ce9gZWRkZGRknOYBAYCapubfwVq1atWWLVtGjx79SQvi8XhpaWlubm51TgUA1GDpegdr3759W7ZsSWxv3bq1tLS0cePGF1xwQUFBwfbt2+fNm5dc+fjjj/fq1atLly5VT580aVLv3r3btWtXWVk5Y8aM0tLSWbNmVesFAAA1V7oG1vr166+88srE9oQJE6Io+vrXv15YWFheXr5t27bksj179hQVFU2fPv2Y03fv3n3rrbdWVFTk5OTk5eWtXr26Z8+e1TY8AFCzxeLxeKpnSA+VlZU5OTl79uzJzs5O9SxAdftpyc5UjwDV5/t5TcI+YS38GVrz34MFAFDNBBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgsHQNrNWrV19zzTUtWrSIxWLPPffcCdesXLky9rfefPPN5NGioqJOnTplZGR06tTp2Wefraa5AYBaIF0Da//+/V27dn3kkUf+7sq33nqr/GPt2rVL7CwuLh4xYsTIkSM3btw4cuTI4cOHv/baa6d5ZACgtqiX6gFO0pAhQ4YMGfJZVp533nmNGjU6Zue0adMGDhxYUFAQRVFBQcGqVaumTZs2f/784HMCALVQut7B+uzy8vJyc3P79++/YsWK5M7i4uJBgwYlHw4ePHjdunXHn3vgwIHKKqpjXAAg/dXkwMrNzZ07d25RUdGiRYs6dOjQv3//1atXJw5VVFQ0a9YsubJZs2YVFRXHP8PkyZNzPtaqVatqmhsASHPp+hLhZ9GhQ4cOHToktvv06VNWVvbggw9eccUViT2xWCy5Mh6PV32YVFBQMGHChMR2ZWWlxgIAPouafAfrGL179968eXNiu3nz5lVvWb333ntVb2glZWRkZFdRTYMCAGmuFgVWSUlJbm5uYrtPnz7Lly9PHlq2bFl+fn6K5gIAapp0fYlw3759W7ZsSWxv3bq1tLS0cePGF1xwQUFBwfbt2+fNmxdF0bRp0y688MLOnTsfPHjwl7/8ZVFRUVFRUeKUcePGXXHFFVOmTLn22muff/75l156ae3atSm7GACgZknXwFq/fv2VV16Z2E68TerrX/96YWFheXn5tm3bEvsPHjw4ceLE7du3Z2Vlde7c+be//e3QoUMTh/Lz859++ukf/OAH99xzT9u2bRcsWNCrV6+UXAgAUPPE4vF4qmdID5WVlTk5OXv27PFmLKiFflqyM9UjQPX5fl6TsE9YC3+G1qL3YAEAVA+BBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQWLoG1urVq6+55poWLVrEYrHnnnvuhGsWLVo0cODApk2bZmdn9+nT58UXX0weKiwsjP2tjz76qJpGBwBqunQNrP3793ft2vWRRx75lDWrV68eOHDgkiVLNmzYcOWVV15zzTUlJSXJo9nZ2eVVZGZmnv6pAYBaoV6qBzhJQ4YMGTJkyKevmTZtWnL7/vvvf/7551944YW8vLzEnlgs1rx589M3IQBQa6XrHax/1NGjR/fu3du4cePknn379rVu3fr888+/+uqrq97ZAgA4RbUlsB566KH9+/cPHz488fDiiy8uLCxcvHjx/PnzMzMz+/btu3nz5uPPOnDgQGUV1TsyAJCu0vUlwn/I/Pnzf/zjHz///PPnnXdeYk/v3r179+6d2O7bt+9ll102c+bMGTNmHHPi5MmTJ02aVK2zAgDpr+bfwVqwYMHo0aMXLlw4YMCAEy6oU6fO5ZdffsI7WAUFBXs+VlZWdponBQBqiBp+B2v+/Pnf+MY35s+f/4UvfOGT1sTj8dLS0ksuueT4QxkZGRkZGadzQACgBkrXwNq3b9+WLVsS21u3bi0tLW3cuPEFF1xQUFCwffv2efPmRVE0f/78UaNGTZ8+vXfv3hUVFVEUZWVl5eTkRFE0adKk3r17t2vXrrKycsaMGaWlpbNmzUrh5QAANUm6vkS4fv36vLy8xGcuTJgwIS8v74c//GEUReXl5du2bUuseeyxxw4fPjx27Njcj40bNy5xaPfu3bfeemvHjh0HDRq0ffv21atX9+zZM1XXAgDUMLF4PJ7qGdJDZWVlTk7Onj17srOzUz0LUN1+WrIz1SNA9fl+XpOwT1gLf4am6x0sAIAzlsACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAgsxYFVVlb2l7/8JbH9+uuvjx8/fu7cuakdCQDgFKU4sL72ta+tWLEiiqKKioqBAwe+/vrrd91117333pvaqQAATkWKA+t//ud/evbsGUXRwoULu3Tpsm7duqeeeqqwsDC1UwEAnIoUB9ahQ4cyMjKiKHrppZe++MUvRlF08cUXl5eXp3YqAIBTkeLA6ty585w5c9asWbN8+fKrrroqiqIdO3ace+65qZ0KAOBUpDiwpkyZ8thjj33+85//6le/2rVr1yiKFi9enHjREAAgTdVL7Zf//Oc/v3PnzsrKynPOOSex59Zbb/2nf/qn1E4FAHAqUv85WPF4fMOGDY899tjevXujKKpfv77AAgDSWorvYL377rtXXXXVtm3bDhw4MHDgwIYNGz7wwAMfffTRnDlzUjsYAMBJS/EdrHHjxvXo0WPXrl1ZWVmJPdddd93LL7+c2qkAAE5Fiu9grV279tVXX61fv35yT+vWrbdv357CkQAATlGK72AdPXr0yJEjVff85S9/adiwYarmAQA4dSkOrIEDB06bNi2xHYvF9u3b96Mf/Wjo0KEpHQoA4JSk+CXChx9++Morr+zUqdNHH330ta99bfPmzU2aNJk/f35qpwIAOBUpDqwWLVqUlpbOnz//jTfeOHr06OjRo6+//vrkG94BANJRigMriqKsrKxvfOMb3/jGN1I9CABAGCkOrHnz5p1w/6hRo6p5EgCAUFIcWOPGjUtuHzp06IMPPkh8krvAAgDSV4p/i3BXFfv27Xvrrbc+97nPeZM7AJDWUv+3CKtq167dT3/606q3tQAA0s6ZFVhRFNWtW3fHjh2pngIA4OSl+D1YixcvTm7H4/Hy8vJHHnmkb9++KRwJAOAUpTiwhg0bltyOxWJNmzb913/914ceeih1EwEAnKoUB9bRo0dTOwAAQHBn3HuwAADSXWruYE2YMOHTF0ydOvXTF6xevfpnP/vZhg0bysvLn3322aovNVa1atWqCRMm/OEPf2jRosWdd945ZsyY5KGioqJ77rnn7bffbtu27U9+8pPrrrvuH7wIAIATS01glZSUfMrRWCz2d59h//79Xbt2vemmm770pS990pqtW7cOHTr0lltu+eUvf/nqq6/edtttTZs2TawvLi4eMWLEfffdd9111z377LPDhw9fu3Ztr169TuJaAACOEYvH46me4ZTEYrFPuoP1ve99b/HixZs2bUo8HDNmzMaNG4uLi6MoGjFiRGVl5dKlSxOHrrrqqnPOOefTP+C0srIyJydnz5492dnZga8BOOP9tGRnqkeA6vP9vCZhn7AW/gytye/BKi4uHjRoUPLh4MGD169ff+jQoRMeWrduXQpGBABqohT/FmEURb/73e+eeeaZbdu2HTx4MLlz0aJFp/7MFRUVzZo1Sz5s1qzZ4cOHd+7cmZube/yhioqK45/hwIEDBw4cSGxXVlae+kgAQG2Q4sB6+umnR40aNWjQoOXLlw8aNGjz5s0VFRUB329e9e1ciRdDk3uOOXTCN35Nnjx50qRJoYY5Ia87UKsEf90B4MyU4pcI77///ocffvg3v/lN/fr1p0+fvmnTpuHDh19wwQVBnrx58+ZV70u999579erVO/fcc094qOoNraSCgoI9HysrKwsyFQBQ46U4sN5+++0vfOELURRlZGTs378/Fot95zvfmTt3bpAn79Onz/Lly5MPly1b1qNHj7POOuuEh/Lz849/hoyMjOwqgkwFANR4KQ6sxo0b7927N4qili1b/s///E8URbt37/7ggw/+7on79u0rLS0tLS2Nomjr1q2lpaXbtm2LoqigoGDUqFGJNWPGjHn33XcnTJiwadOmJ5544vHHH584cWLi0Lhx45YtWzZlypQ333xzypQpL7300vjx40/LFQIAtU/KAivRRv369UvcSRo+fPi4ceNuueWWr371q/379/+7p69fvz4vLy8vLy+KogkTJuTl5f3whz+Moqi8vDxRWlEUtWnTZsmSJStXruzWrdt99903Y8aM5Idm5efnP/30008++eSll15aWFi4YMECH4IFAISSss/BqlOnTl5e3rBhw26++ebc3NyjR48++OCDa9eu/ed//ud77rnnnHPOSclUn+I0fYaHN7lTq6Tvm9x9q1Kr+BysU5eyO1ivvvrqZZdd9uCDD7Zt2/aGG25YtWrVnXfeuXjx4qlTp56BdQUA8NmlLLD69Onz85//vKKi4tFHH/3LX/4yYMCAxN8E/Mtf/pKqkQAAgkjxm9yzsrK+/vWvr1y58k9/+tNXv/rVxx57rE2bNkOHDk3tVAAAp+JM+VM5bdu2/f73v3/33XdnZ2e/+OKLqR4HAODkpf5P5URRtGrVqieeeKKoqKhu3brDhw8fPXp0qicCADh5qQyssrKywsLCwsLCrVu35ufnz5w5c/jw4Q0aNEjhSAAApy5lgTVw4MAVK1Y0bdp01KhR3/jGNzp06JCqSQAAwkpZYGVlZRUVFV199dV169ZN1QwAAKdDygJr8eLFqfrSAACn1ZnyW4QAADWGwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACCyNA2v27Nlt2rTJzMzs3r37mjVrjl9w4403xv5W586dE4cKCwuPOfTRRx9V7/gAQI2VroG1YMGC8ePH33333SUlJf369RsyZMi2bduOWTN9+vTyj5WVlTVu3PgrX/lK8mh2dnZ5FZmZmdV7BQBAjZWugTV16tTRo0fffPPNHTt2nDZtWqtWrR599NFj1uTk5DT/2Pr163ft2nXTTTclj8ZiseZVVO/4AEBNlpaBdfDgwQ0bNgwaNCi5Z9CgQevWrfuUUx5//PEBAwa0bt06uWffvn2tW7c+//zzr7766pKSktM4LgBQy9RL9QAnY+fOnUeOHGnWrFlyT7NmzSoqKj5pfXl5+dKlS5966qnknosvvriwsPCSSy6prKycPn163759N27c2K5du2NOPHDgwIEDBxLblZWVQS8CAKix0vIOVkIsFktux+Pxqg+PUVhY2KhRo2HDhiX39O7d+4YbbujatWu/fv0WLlzYvn37mTNnHn/i5MmTcz7WqlWroOMDADVWWgZWkyZN6tatW/WW1XvvvVf1hlZV8Xj8iSeeGDlyZP369U+4oE6dOpdffvnmzZuPP1RQULDnY2VlZUGGBwBqvLQMrPr163fv3n358uXJPcuXL8/Pzz/h4lWrVm3ZsmX06NGf9GzxeLy0tDQ3N/f4QxkZGdlVnPrkAEBtkJbvwYqiaMKECSNHjuzRo0efPn3mzp27bdu2MWPGRFFUUFCwffv2efPmJVc+/vjjvXr16tKlS9XTJ02a1Lt373bt2lVWVs6YMaO0tHTWrFnVfQ0AQA2VroE1YsSI999//9577y0vL+/SpcuSJUsSvyFYXl5e9QOx9uzZU1RUNH369GNO371796233lpRUZGTk5OXl7d69eqePXtW6wUAADVXLB6Pp3qG9FBZWZmTk7Nnz56wrxX+tGRnwGeDM9z385qkeoST5FuVWiX4t+pp+hl6JkvL92ABAJzJBBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgsDQOrNmzZ7dp0yYzM7N79+5r1qw5fsHKlStjf+vNN99MHi0qKurUqVNGRkanTp2effbZahwcAKjh0jWwFixYMH78+LvvvrukpKRfv35DhgzZtm3bCVe+9dZb5R9r165dYmdxcfGIESNGjhy5cePGkSNHDh8+/LXXXqvG8QGAmiwWj8dTPcPJ6NWr12WXXfboo48mHnbs2HHYsGGTJ0+uumblypVXXnnlrl27GjVqdMzpI0aMqKysXLp0aeLhVVdddc4558yfP/9TvmJlZWVOTs6ePXuys7ODXUYU/bRkZ8BngzPc9/OapHqEk+RblVol+LfqafoZeiZLyztYBw8e3LBhw6BBg5J7Bg0atG7duhMuzsvLy83N7d+//4oVK5I7i4uLq54+ePDgE55+4MCByirCXQEAUJOlZWDt3LnzyJEjzZo1S+5p1qxZRUXFMctyc3Pnzp1bVFS0aNGiDh069O/ff/Xq1YlDFRUVf/f0KIomT56c87FWrVqdhksBAGqgeqke4OTFYrHkdjwer/owoUOHDh06dEhs9+nTp6ys7MEHH7ziiis+4+lRFBUUFEyYMCGxXVlZqbEAgM8iLe9gNWnSpG7dulXvOb333ntV70idUO/evTdv3pzYbt68+Wc5PSMjI7uKELMDADVfWgZW/fr1u3fvvnz58uSe5cuX5+fnf/pZJSUlubm5ie0+ffpUPX3ZsmV/93QAgM8oXV8inDBhwsiRI3v06NGnT5+5c+du27ZtzJgxURQVFBRs37593rx5URRNmzbtwgsv7Ny588GDB3/5y18WFRUVFRUlTh83btwVV1wxZcqUa6+99vnnn3/ppZfWrl2byusBAGqQdA2sESNGvP/++/fee295eXmXLl2WLFnSunXrKIrKy8uTH4h18ODBiRMnbt++PSsrq3Pnzr/97W+HDh2aOJSfn//000//4Ac/uOeee9q2bbtgwYJevXql7GIAgJolXT8Hq/r5HCw4dT4HC9KCz8E6dWn5HiwAgDOZwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAEJrAAAAITWAAAgQksAIDABBYAQGACCwAgMIEFABCYwAIACExgAQAElsaBNXv27DZt2mRmZnbv3n3NmjXHL1i0aNHAgQObNm2anZ3dp0+fF198MXmosLAw9rc++uijapwdAKjJ0jWwFixYMH78+LvvvrukpKRfv35DhgzZtm3bMWtWr149cODAJUuWbNiw4corr7zmmmtKSkqSR7Ozs8uryMzMrN4rAABqrHqpHuAkTZ06dfTo0TfffHMURdOmTXvxxRcfffTRyZMnV10zbdq05Pb999///PPPv/DCC3l5eYk9sVisefPm1TgyAFBbpOUdrIMHD27YsGHQoEHJPYMGDVq3bt2nnHL06NG9e/c2btw4uWffvn2tW7c+//zzr7766qp3tqo6cOBAZRWh5gcAara0DKydO3ceOXKkWbNmyT3NmjWrqKj4lFMeeuih/fv3Dx8+PPHw4osvLiwsXLx48fz58zMzM/v27bt58+bjz5o8eXLOx1q1ahX2KgCAmiotAyshFoslt+PxeNWHx5g/f/6Pf/zjBQsWnHfeeYk9vXv3vuGGG7p27dqvX7+FCxe2b99+5syZx59YUFCw52NlZWXBLwEAqJHS8j1YTZo0qVu3btVbVu+9917VG1pVLViwYPTo0c8888yAAQNOuKBOnTqXX375Ce9gZWRkZGRkBJkZAKg90vIOVv369bt37758+fLknuXLl+fn5x+/cv78+TfeeONTTz31hS984ZOeLR6Pl5aW5ubmnpZZAYDaJy3vYEVRNGHChJEjR/bo0aNPnz5z587dtm3bmDFjoigqKCjYvn37vHnzoiiaP3/+qFGjpk+f3rt378TtrqysrJycnCiKJk2a1Lt373bt2lVWVs6YMaO0tHTWrFmpvSIAoMZI18AaMWLE+++/f++995aXl3fp0mXJkiWtW7eOoqi8vDz5gViPPfbY4cOHx44dO3bs2MSer3/964WFhVEU7d69+9Zbb62oqMjJycnLy1u9enXPnj1TdCkAQE0Ti8fjqZ4hPVRWVubk5OzZsyc7Ozvg0/60ZGfAZ4Mz3PfzmqR6hJPkW5VaJfi36mn6GXomS8v3YAEAnMkEFgBAYAILACAwgQUAEJjAAgAITGABAAQmsAAAAhNYAACBCSwAgMAEFgBAYAILACAwgQUAEJjAAgAITGABAAQmsAAAAhNYAACBCSwAgMAEFgBAYAILACAwgQUAEJjAAgAITGABAAQmsAAAAhNYAACBCSwAgMAEFgBAYAILACAwgQUAEJjAAgAITGABAAQmsAAAAhNYAACBCSwAgMAEFgBAYGkcWLNnz27Tpk1mZmb37t3XrFlzwjWrVq3q3r17ZmbmRRddNGfOnKqHioqKOnXqlJGR0alTp2effbZaRgYAaoV0DawFCxaMHz/+7rvvLikp6dev35AhQ7Zt23bMmq1btw4dOrRfv34lJSV33XXXHXfcUVRUlDhUXFw8YsSIkSNHbty4ceTIkcOHD3/ttdeq/SIAgJopFo/HUz3DyejVq9dll1326KOPJh527Nhx2LBhkydPrrrme9/73uLFizdt2pR4OGbMmI0bNxYXF0dRNGLEiMrKyqVLlyYOXXXVVeecc878+fM/5StWVlbm5OTs2bMnOzs74IX8tGRnwGeDM9z385qkeoST5FuVWiX4t+pp+hl6JkvLO1gHDx7csGHDoEGDknsGDRq0bt26Y5YVFxdXXTN48OD169cfOnTohIeOPx0A4OTUS/UAJ2Pnzp1Hjhxp1qxZck+zZs0qKiqOWVZRUXHMmsOHD+/cuTM3N/f4Q8efHkXRgQMHDhw4kNjes2dPFEWVlZUBLySKoo/27Q37hHAmq6ysn+oRTpJvVWqV4N+qiZ+eafqi2clJy8BKiMViye14PF714Setqbrns5w+efLkSZMmVd3TqlWrU5saarVJf38JkHqn6Vt17969OTk5p+e5zzhpGVhNmjSpW7du1XtO7733XtU7UgnNmzc/Zk29evXOPffcEx46/vQoigoKCiZMmJDYPnr06F//+tdzzz33hClGGqmsrGzVqlVZWVnteSsApCnfrTVGPB7fu3dvixYtUj1I9UnLwKpfv3737t2XL19+3XXXJfYsX7782muvPWZZnz59XnjhheTDZcuW9ejR46yzzkocWr58+Xe+853kofz8/OO/UEZGRkZGRvJho0aNgl4HqZSdne0/2ZAWfLfWDLXn3lVCWgZWFEUTJkwYOXJkjx49+vTpM3fu3G3bto0ZMyaKooKCgu3bt8+bNy+KojFjxjzyyCMTJky45ZZbiouLH3/88eTvCY4bN+6KK66YMmXKtdde+/zzz7/00ktr165N5fUAADVIugbWiBEj3n///Xvvvbe8vLxLly5Llixp3bp1FEXl5eXJD8Rq06bNkiVLvvOd78yaNatFixYzZsz40pe+lDiUn5//9NNP/+AHP7jnnnvatm27YMGCXr16pexiAICaJV0/BwtO2oEDByZPnlxQUFD19V/gDOS7lfQlsAAAAkvLDxoFADiTCSwAgMAEFgBAYAILgPTwzjvvxGKx0tLSKIpWrlwZi8V2796d4pngE6TrxzQAUNu0atWqvLy8SZMmqR4E/j6BBUB6qFu3bvPmzVM9BXwmXiIkvcXj8QceeOCiiy7Kysrq2rXrr3/96yiKdu3adf311zdt2jQrK6tdu3ZPPvlkFEUHDx789re/nZubm5mZeeGFF06ePDnxDFOnTr3kkksaNGjQqlWr2267bd++fVEU7d+/Pzs7O/FsCS+88EKDBg327t2biquEmuzXv/71JZdckpWVde655w4YMGD//v1Hjx699957zz///IyMjG7duv3Xf/1XYmXVlwjhDOcOFuntBz/4waJFix599NF27dqtXr36hhtuaNq06TPPPPPHP/5x6dKlTZo02bJly4cffhhF0YwZMxYvXrxw4cILLrigrKysrKws8Qx16tSZMWPGhRdeuHXr1ttuu+3OO++cPXt2gwYN/v3f//3JJ5/88pe/nFiW2G7YsGHKLhVqovLy8q9+9asPPPDAddddt3fv3jVr1sTj8enTpz/00EOPPfZYXl7eE0888cUvfvEPf/hDu3btUj0s/AN80ChpbP/+/U2aNHnllVf69OmT2HPzzTd/8MEH+/bta9KkyRNPPFF18R133PGHP/zhpZdeisVin/SEzzzzzLe+9a2dO3dGUfT666/n5+dv27atRYsWO3fubNGixfLly//lX/7ltF4R1DZvvPFG9+7d33nnncSfO0to2bLl2LFj77rrrsTDnj17Xn755bNmzXrnnXfatGlTUlLSrVu3lStXXnnllbt27WrUqFFqRodP5SVC0tgf//jHjz76aODAgWd/bN68eW+//fa3vvWtp59+ulu3bnfeeee6desSi2+88cbS0tIOHTrccccdy5YtSz7JihUrBg4c2LJly4YNG44aNer999/fv39/FEU9e/bs3Llz4g+H/+IXv7jggguuuOKKlFwm1GBdu3bt37//JZdc8pWvfOXnP//5rl27Kisrd+zY0bdv3+Savn37btq0KYVDwkkQWKSxo0ePRlH029/+tvRjf/zjH3/9618PGTLk3XffHT9+/I4dO/r37z9x4sQoii677LKtW7fed999H3744fDhwxOv/b377rtDhw7t0qVLUVHRhg0bZs2aFUXRoUOHEs9/8803J96/9eSTT950002fcusLODl169Zdvnz50qVLO3XqNHPmzA4dOmzdujWKoqrfbvF43Hcf6ScOaauysjIjI2PevHmfsmbOnDkNGzY8ZmfiPbPvv//+r3/963r16h05ciSx/7777ouiaNeuXYmHf/3rXzMzM6dPn16nTp2ysrLTcAXA/+/w4cMtW7Z86KGHWrRo8ZOf/CS5//LLLx87dmw8Hk+0V0lJSTweX7FiRdXvVjjTeJM7aaxhw4YTJ078zne+c/To0c997nOVlZXr1q07++yz33777e7du3fu3PnAgQO/+c1vOnbsGEXRww8/nJub261btzp16jzzzDPNmzdv1KhR27ZtDx8+PHPmzGuuuebVV1+dM2dO1ec/55xz/u3f/u273/3uoEGDzj///BRdJdRkr7322ssvvzxo0KDzzjvvtdde+7//+7+OHTt+97vf/dGPftS2bdtu3bo9+eSTpaWlv/rVr1I9KfyDUl14cEqOHj06ffr0Dh06nHXWWU2bNh08ePCqVavuu+++jh07ZmVlNW7c+Nprr/3zn/8cj8fnzp3brVu3Bg0aZGdn9+/f/4033kg8w9SpU3Nzc7OysgYPHpx4x1XV/yd++eWXoyhauHBhSq4Oarw//vGPgwcPbtq0aUZGRvv27WfOnBmPx48cOTJp0qSWLVueddZZXbt2Xbp0aWKxO1ikEb9FCJ/mV7/61bhx43bs2FG/fv1UzwJA2vASIZzYBx98sHXr1smTJ3/zm99UVwD8Q/wWIZzYAw880K1bt2bNmhUUFKR6FgDSjJcIAQACcwcLACAwgQUAEJjAAgAITGABAAQmsIBabeXKlbFYbPfu3akeBKhRBBZwelVUVNx+++0XXXRRRkZGq1atrrnmmsTn43+SwsLCRo0aVdd0UX5+fnl5eU5OTrV9RaA28EGjwGn0zjvv9O3bt1GjRg888MCll1566NChF198cezYsW+++WaqR4uiKDp06FD9+vWbN2+e6kGAmsYdLOA0uu2222Kx2Ouvv/7lL3+5ffv2nTt3njBhwn//939HUTR16tRLLrmkQYMGrVq1uu222/bt2xdF0cqVK2+66aY9e/bEYrFYLPbjH/84iqKDBw/eeeedLVu2bNCgQa9evVauXJl8/p///OetWrX6p3/6p+uuu27q1KlVb309+uijbdu2rV+/focOHX7xi18k98disTlz5lx77bUNGjT4j//4j2NeIly3bt0VV1yRlZXVqlWrO+64Y//+/Yn9s2fPbteuXWZmZrNmzb785S+f3n81oAZI9R9DBGqs999/PxaL3X///Sc8+vDDD7/yyit//vOfX3755Q4dOnzrW9+Kx+MHDhyYNm1adnZ2eXl5eXn53r174/H41772tfz8/NWrV2/ZsuVnP/tZRkbGn/70p3g8vnbt2jp16vzsZz976623Zs2a1bhx45ycnMSTL1q06Kyzzpo1a9Zbb7310EMP1a1b95VXXkkciqLovPPOe/zxx99+++133nmn6t8M/v3vf3/22Wc//PDDf/rTn1599dW8vLwbb7wxHo//7ne/q1u37lNPPfXOO++88cYb06dPP93/dEC6E1jA6fLaa69FUbRo0aK/u3LhwoXnnntuYvvJJ59MdlI8Ht+yZUssFtu+fXtyT//+/QsKCuLx+IgRI77whS8k919//fXJE/Pz82+55Zbkoa985StDhw5NbEdRNH78+OShqoE1cuTIW2+9NXlozZo1derU+fDDD4uKirKzsysrKz/rlQO1npcIgdMlHo9HURSLxU54dMWKFQMHDmzZsmXDhg1HjRr1/vvvJ1+Pq+qNN96Ix+Pt27c/+2OrVq16++23oyh66623evbsmVxZdXvTpk19+/ZNPuzbt++mTZuSD3v06HHCkTZs2FBYWJj8QoMHDz569OjWrVsHDhzYunXriy66aOTIkb/61a8++OCDf+wfAqh9vMkdOF3atWsXi8U2bdo0bNiwYw69++67Q4cOHTNmzH333de4ceO1a9eOHj360KFDxz/J0aNH69atu2HDhrp16yZ3nn322VEUxePxqvUW/9u/rHrMoaoPGzRocMKBjx49+s1vfvOOO+6ouvOCCy6oX7/+G2+8sXLlymXLlv3whz/88Y9//Lvf/a46f9URSDvuYAGnS+PGjQcPHjxr1qxjbk3t3r17/fr1hw8ffuihh3r37t2+ffsdO3Ykj9avX//IkSPJh3l5eUeOHHnvvff+uYrE7/1dfPHFr7/+enLl+vXrk9sdO3Zcu3Zt8uG6des6duz4dwe+7LLL/vCHP/zz36pfv34URfXq1RswYMADDzzw+9///p133nnllVdO5l8EqDUEFnAazZ49+8iRIz179iwqKtq8efOmTZtmzJjRp0+ftm3bHj58eObMmX/+859/8YtfzJkzJ3nKhRdeuG/fvpdffnnnzp0ffPBB+/btr7/++lGjRi1atGjr1q2/+93vpkyZsmTJkiiKbr/99iVLlkydOnXz5s2PPfbY0qVLk7epvvvd7xYWFs6ZM2fz5s1Tp05dtGjRxIkT/+603/ve94qLi8eOHVtaWrp58+bFixfffvvtURT95je/mTFjRmlp6bvvvjtv3ryjR4926NDh9PyDATVFSt8BBtR8O3bsGDt2bOvWrevXr9+yZcsvfvGLK1asiMfjU6dOzc3NzcrKGjx48Lx586KP32kej8fHjBlz7rnnRlH0ox/9KB6PHzx48Ic//OGFF1541llnNW/e/Lrrrvv973+fWDl37tyWLVtmZWUNGzbsP/7jP5o3b578urNnz77ooovOOuus9u3bz5s3L7k/iqJnn302+bDqm9zj8fjrr78+cODAs88+u0GDBpdeeulPfvKTeDy+Zs2af/mXfznnnHOysrIuvfTSBQsWnLZ/LaCGiMX/9l0LAGnqlltuefPNN9esWZPqQQC8yR1IZw8++ODAgQMbNGiwdOnS//zP/5w9e3aqJwKIoihyBwtIY8OHD1+5cuXevXsvuuii22+/fcyYMameCCCKBBYAQHB+ixAAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAIDCBBQAQmMACAAhMYAEABCawAAACE1gAAIEJLACAwAQWAEBgAgsAILD/D5cLOzmesROtAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check its corresponding caption:"
      ],
      "metadata": {
        "id": "v-xN4HbsM5ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example[\"code\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "n4WLPzYlutg6",
        "outputId": "03f033fe-6966-4f61-f922-1b5b7dbcd37d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport matplotlib.pyplot as plt\\n\\n# Categories and their corresponding values\\ncategories = ['essay', 'soil']\\nvalues = [1, 2]\\n\\n# Creating the bar chart\\nplt.figure(figsize=(8, 5))  # Set the figure size (optional)\\nplt.bar(categories, values, color='skyblue')  # Plot the bars with skyblue color\\n\\n# Adding title and labels\\nplt.title('Title')  # Add a title to the chart\\nplt.xlabel('Categories')  # Label for the x-axis\\nplt.ylabel('Values')  # Label for the y-axis\\n\\n# Display the chart\\nplt.show()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create PyTorch Dataset\n",
        "\n",
        "Next, we create a standard [PyTorch dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). Each item of the dataset returns the expected inputs for the model, in this case input_ids, attention_mask and pixel_values.\n",
        "\n",
        "We use `GitProcessor` to turn each (image, text) pair into the expected inputs. Basically, the text gets turned into `input_ids` and `attention_mask`, and the image gets turned into `pixel_values`."
      ],
      "metadata": {
        "id": "sSWkkqiKqrhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "prompt = (\n",
        "        \"Convert the figure you are given into a full code. Here is an example of the expected output: <N>\"\n",
        "        \"import matplotlib.pyplot as plt <N>\"\n",
        "        \"# Categories and their corresponding values<N>\"\n",
        "        \"categories = ['essay', 'soil']<N>\"\n",
        "        \"values = [1, 2]<N>\"\n",
        "        \"# Creating the bar chart<N>\"\n",
        "        \"plt.figure(figsize=(8, 5))  # Set the figure size (optional)<N>\"\n",
        "        \"plt.bar(categories, values, color='skyblue')  # Plot the bars with skyblue color<N>\"\n",
        "        \"# Adding title and labels<N>\"\n",
        "        \"plt.title('Title')  # Add a title to the chart<N>\"\n",
        "        \"plt.xlabel('Categories')  # Label for the x-axis<N>\"\n",
        "        \"plt.ylabel('Values')  # Label for the y-axis<N>\"\n",
        "        \"# Display the chart<N>\"\n",
        "        \"plt.show()\"\n",
        "    )\n",
        "\n",
        "class ImageCaptioningDataset(Dataset):\n",
        "    def __init__(self, dataset, processor):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "\n",
        "        encoding = self.processor(images=item[\"image\"], text=prompt, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        label = self.processor(text=item[\"code\"], padding=\"max_length\", return_tensors=\"pt\")\n",
        "        # remove batch dimension\n",
        "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
        "        label = {k: v.squeeze() for k,v in label.items()}\n",
        "        return encoding, label, item[\"code\"]"
      ],
      "metadata": {
        "id": "93od71o_qq_V"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-vqav2\")"
      ],
      "metadata": {
        "id": "hLhbdBLNxBuF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = ImageCaptioningDataset(dataset_test, processor)\n",
        "train_dataset = ImageCaptioningDataset(dataset_train, processor)\n",
        "val_dataset = ImageCaptioningDataset(dataset_val, processor)"
      ],
      "metadata": {
        "id": "hxajSwc3w-LU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check one example of the dataset:"
      ],
      "metadata": {
        "id": "mFvbr5nZN4Mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "item, label, code = test_dataset[0]\n",
        "for k,v in item.items():\n",
        "  print(k,v.shape)\n",
        "for k,v in label.items():\n",
        "  print(k,v.shape)\n",
        "print(code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D36xWH33xGSF",
        "outputId": "279a9e2d-918f-4f21-ee41-08bae7a7de89"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids torch.Size([512])\n",
            "attention_mask torch.Size([512])\n",
            "pixel_values torch.Size([3, 480, 480])\n",
            "input_ids torch.Size([512])\n",
            "attention_mask torch.Size([512])\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# Categories and their corresponding values\n",
            "categories = ['pain', 'stock', 'graph']\n",
            "values = [7, 2, 2]\n",
            "\n",
            "# Creating the bar chart\n",
            "plt.figure(figsize=(8, 5))  # Set the figure size (optional)\n",
            "plt.bar(categories, values, color='skyblue')  # Plot the bars with skyblue color\n",
            "\n",
            "# Adding title and labels\n",
            "plt.title('Accuracy of different algorithms')  # Add a title to the chart\n",
            "plt.xlabel('Categories')  # Label for the x-axis\n",
            "plt.ylabel('Accuracy')  # Label for the y-axis\n",
            "\n",
            "# Display the chart\n",
            "plt.show()\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create PyTorch DataLoader\n",
        "\n",
        "Next, we create a corresponding [PyTorch DataLoader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html), which allows us to get batches of data from the dataset.\n",
        "\n",
        "We need this as neural networks (like GIT) are trained on batches of data, using stochastic gradient descent (SGD)."
      ],
      "metadata": {
        "id": "DReYWvkSyKEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=4)\n",
        "val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size=4)\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=4)"
      ],
      "metadata": {
        "id": "HApEzWi6yLgl"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch, label, code = next(iter(train_dataloader))\n",
        "for k,v in batch.items():\n",
        "  print(k,v.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzJMeNNnyQsR",
        "outputId": "9937ae51-9798-499a-a083-b731a02eb95c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids torch.Size([4, 512])\n",
            "attention_mask torch.Size([4, 512])\n",
            "pixel_values torch.Size([4, 3, 480, 480])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chPVVDXF7in6",
        "outputId": "1c222472-5b82-41f4-889c-38cc55bcd571"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"\\nimport matplotlib.pyplot as plt\\n\\n# Categories and their corresponding values\\ncategories = ['image', 'bath', 'area', 'tissue', 'ideal', 'vein']\\nvalues = [1000000000, 1000000000, 10000, 1000000000, 10, 10000000]\\n\\n# Creating the bar chart\\nplt.figure(figsize=(8, 5))  # Set the figure size (optional)\\nplt.bar(categories, values, color='skyblue')  # Plot the bars with skyblue color\\n\\n# Adding title and labels\\nplt.title('Title')  # Add a title to the chart\\nplt.xlabel('Categories')  # Label for the x-axis\\nplt.ylabel('Values')  # Label for the y-axis\\n\\n# Display the chart\\nplt.show()\\n\", \"\\nimport matplotlib.pyplot as plt\\n\\n# Categories and their corresponding values\\ncategories = ['bomb', 'game', 'scope', 'roof', 'run']\\nvalues = [3, 5, 1, 2, 8]\\n\\n# Creating the bar chart\\nplt.figure(figsize=(8, 5))  # Set the figure size (optional)\\nplt.bar(categories, values, color='skyblue')  # Plot the bars with skyblue color\\n\\n# Adding title and labels\\nplt.title('Most preferred objects')  # Add a title to the chart\\nplt.xlabel('Categories')  # Label for the x-axis\\nplt.ylabel('Number of People')  # Label for the y-axis\\n\\n# Display the chart\\nplt.show()\\n\", \"\\nimport matplotlib.pyplot as plt\\n\\n# Categories and their corresponding values\\ncategories = ['spread', 'hair', 'train']\\nvalues = [9, 5, 8]\\n\\n# Creating the bar chart\\nplt.figure(figsize=(8, 5))  # Set the figure size (optional)\\nplt.bar(categories, values, color='skyblue')  # Plot the bars with skyblue color\\n\\n# Adding title and labels\\nplt.title('Accuracy of different algorithms')  # Add a title to the chart\\nplt.xlabel('Categories')  # Label for the x-axis\\nplt.ylabel('Accuracy')  # Label for the y-axis\\n\\n# Display the chart\\nplt.show()\\n\", \"\\nimport matplotlib.pyplot as plt\\n\\n# Categories and their corresponding values\\ncategories = ['plant', 'home', 'boy', 'phrase', 'salary', 'skin', 'flight']\\nvalues = [8, 1, 6, 2, 4, 1, 9]\\n\\n# Creating the bar chart\\nplt.figure(figsize=(8, 5))  # Set the figure size (optional)\\nplt.bar(categories, values, color='skyblue')  # Plot the bars with skyblue color\\n\\n# Adding title and labels\\nplt.title('Accuracy of different algorithms')  # Add a title to the chart\\nplt.xlabel('Categories')  # Label for the x-axis\\nplt.ylabel('Accuracy')  # Label for the y-axis\\n\\n# Display the chart\\nplt.show()\\n\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check one batch, and do some sanity checks. We can decode the input_ids back into text for instance:"
      ],
      "metadata": {
        "id": "A2lEDYRhOKsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processor.decode(batch[\"input_ids\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "9yxdjyYdOOYa",
        "outputId": "1fefb55e-0a4a-4fb1-ae51-8b7fd761a0fc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[CLS] convert the figure you are given into a full code. here is an example of the expected output : < n > import matplotlib. pyplot as plt < n > # categories and their corresponding values < n > categories = ['essay ','soil'] < n > values = [ 1, 2 ] < n > # creating the bar chart < n > plt. figure ( figsize = ( 8, 5 ) ) # set the figure size ( optional ) < n > plt. bar ( categories, values, color ='skyblue') # plot the bars with skyblue color < n > # adding title and labels < n > plt. title ('title') # add a title to the chart < n > plt. xlabel ('categories') # label for the x - axis < n > plt. ylabel ('values') # label for the y - axis < n > # display the chart < n > plt. show ( ) [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor.decode(label[\"input_ids\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "MP2E8KdaRa6u",
        "outputId": "7a931986-a4ec-47f7-fd04-574f143212a1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[CLS] import matplotlib. pyplot as plt # categories and their corresponding values categories = ['image ','bath ','area ','tissue ','ideal ','vein'] values = [ 1000000000, 1000000000, 10000, 1000000000, 10, 10000000 ] # creating the bar chart plt. figure ( figsize = ( 8, 5 ) ) # set the figure size ( optional ) plt. bar ( categories, values, color ='skyblue') # plot the bars with skyblue color # adding title and labels plt. title ('title') # add a title to the chart plt. xlabel ('categories') # label for the x - axis plt. ylabel ('values') # label for the y - axis # display the chart plt. show ( ) [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can \"denormalize\" the pixel values to get back an image:"
      ],
      "metadata": {
        "id": "tLLSk6u9OU3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
        "STD = np.array([58.395, 57.120, 57.375]) / 255\n",
        "\n",
        "unnormalized_image = (batch[\"pixel_values\"][0].numpy() * np.array(STD)[:, None, None]) + np.array(MEAN)[:, None, None]\n",
        "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
        "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
        "print(unnormalized_image.shape)\n",
        "Image.fromarray(unnormalized_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "xHNEPhZoOW6f",
        "outputId": "615acff1-3087-483b-c8a6-04dee18be7e1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(480, 480, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=480x480>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks ok! Always important to check your data ;) see [this blog](http://karpathy.github.io/2019/04/25/recipe/) for a great overview of tips when training neural networks."
      ],
      "metadata": {
        "id": "b1tkXYKHOpj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define model\n",
        "\n",
        "Next, we instantiate a model. We start from the pre-trained GIT-base model (which was already pre-trained on 4 million image-text pairs by Microsoft).\n",
        "\n",
        "Of course, feel free to start fine-tuning another GIT model from the [hub](https://huggingface.co/models?other=git)."
      ],
      "metadata": {
        "id": "OKiJy-rwxiL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-vqav2\")"
      ],
      "metadata": {
        "id": "kDJ-Y6KIxJWL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dummy forward pass\n",
        "\n",
        "It's always good to check the initial loss on a batch. See also the blog above."
      ],
      "metadata": {
        "id": "VAY30e2LO81P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                pixel_values=batch[\"pixel_values\"],\n",
        "                labels=label[\"input_ids\"])\n",
        "outputs.loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j2wpZDnO8Xe",
        "outputId": "077b3125-acc6-4635-f7ec-df7b94bce932"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(11.4466, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for Eval"
      ],
      "metadata": {
        "id": "sDtAFjLe3HOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from codebleu import calc_codebleu\n",
        "\n",
        "def get_codebleu_scores(targets, preds):\n",
        "    total_codebleu = 0\n",
        "    for index in range(len(targets)):\n",
        "        codebleu_score = calc_codebleu([targets[index]], [preds[index]], lang=\"python\", weights=(0.25, 0.25, 0.25, 0.25), tokenizer=None)\n",
        "        total_codebleu += codebleu_score['codebleu']\n",
        "    return total_codebleu / len(df_targets)\n",
        "\n",
        "# Batch size\n",
        "batch_size_inf = 8\n",
        "\n",
        "# Function to process a batch of images\n",
        "def process_batch(batch_images, prompt, model):\n",
        "    with torch.no_grad():\n",
        "      inputs = processor(images=batch_images, text=[prompt] * len(batch_images), return_tensors=\"pt\", padding=True).to('cuda')\n",
        "      pixel_values = inputs.pixel_values\n",
        "      generated_ids = model.generate(input_ids=inputs.input_ids, pixel_values=pixel_values, max_length=500)\n",
        "      generated_codes = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "    return generated_codes\n",
        "\n",
        "def eval_model(model, dataset_test, prompt):\n",
        "    # List to store the generated captions\n",
        "    codes = []\n",
        "    for i in tqdm(range(0, len(dataset_test), batch_size_inf), desc=\"Evaluating Model\"):\n",
        "        batch_images = [dataset_test[j]['image'] for j in range(i, min(i + batch_size_inf, len(dataset_test)))]\n",
        "        batch_codes = process_batch(batch_images, prompt, model)\n",
        "        codes.extend(batch_codes)\n",
        "    df = pd.DataFrame(codes, columns=[\"code\"])\n",
        "    codebleu_score = get_codebleu_scores(dataset_test['code'], df['code'])\n",
        "    return codebleu_score, df\n"
      ],
      "metadata": {
        "id": "xu2vS_Kn3G0O"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBnCCLgV8sqN",
        "outputId": "f7fbd244-4881-41f6-f63b-a3d11cd63a00"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=800x800>,\n",
              " 'labels': \"['pain', 'stock', 'graph']\",\n",
              " 'values': '[7, 2, 2]',\n",
              " 'title': 'Accuracy of different algorithms',\n",
              " 'value_heading': 'Accuracy',\n",
              " 'code': \"\\nimport matplotlib.pyplot as plt\\n\\n# Categories and their corresponding values\\ncategories = ['pain', 'stock', 'graph']\\nvalues = [7, 2, 2]\\n\\n# Creating the bar chart\\nplt.figure(figsize=(8, 5))  # Set the figure size (optional)\\nplt.bar(categories, values, color='skyblue')  # Plot the bars with skyblue color\\n\\n# Adding title and labels\\nplt.title('Accuracy of different algorithms')  # Add a title to the chart\\nplt.xlabel('Categories')  # Label for the x-axis\\nplt.ylabel('Accuracy')  # Label for the y-axis\\n\\n# Display the chart\\nplt.show()\\n\",\n",
              " 'og_file_name': 'figure_8199.png'}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to('cuda')\n",
        "codebleu_score, df = eval_model(model, dataset_test, prompt)\n",
        "print(codebleu_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "XW6RDBTA7vJi",
        "outputId": "528a201c-4e27-44bc-bc76-9cb48a1dfbee"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Model:   5%|‚ñç         | 6/125 [01:22<27:13, 13.73s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-681c741cb591>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcodebleu_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodebleu_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-ac250ec1d813>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(model, dataset_test, prompt)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_inf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluating Model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mbatch_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size_inf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mbatch_codes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_codes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"code\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-ac250ec1d813>\u001b[0m in \u001b[0;36mprocess_batch\u001b[0;34m(batch_images, prompt, model)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mgenerated_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mgenerated_codes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgenerated_codes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m             \u001b[0;31m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   1825\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2456\u001b[0m         \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_initial_cache_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2458\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_unfinished_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2459\u001b[0m             \u001b[0;31m# prepare model inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2460\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device)\u001b[0m\n\u001b[1;32m   1986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mthis_peer_finished_flag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1988\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1989\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "Next, let's train the model! We use native PyTorch here.\n",
        "\n",
        "As I created a super tiny dataset just for demo purposes, we'll let the model overfit it. If it's capable of overfitting it (i.e. achieve zero loss), then that's a great way to know that everything is working properly. See also the blog above."
      ],
      "metadata": {
        "id": "z_CyLSgBxyL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    train_codebleu = 0\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\")\n",
        "    for idx, (batch, labels, code) in enumerate(progress_bar):\n",
        "        model.train()\n",
        "\n",
        "        input_ids = batch.pop(\"input_ids\").to(device)\n",
        "        attention_mask = batch.pop(\"attention_mask\").to(device)\n",
        "        pixel_values = batch.pop(\"pixel_values\").to(device)\n",
        "        output_ids = labels.pop(\"input_ids\").to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        pixel_values=pixel_values,\n",
        "                        labels=output_ids)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        average_loss = total_loss / num_batches\n",
        "\n",
        "\n",
        "        progress_bar.set_postfix({\"Average Loss\": average_loss})\n",
        "\n",
        "    train_losses.append(average_loss)\n",
        "    # Evaluate on validation set\n",
        "    val_codebleu = evaluate_model(model, val_dataloader)\n",
        "    val_codebleu_scores.append(val_codebleu)\n",
        "    print(f\"Validation CodeBLEU: {val_codebleu}\")\n",
        "\n",
        "    # Save the best model\n",
        "    if val_codebleu > best_val_codebleu:\n",
        "        best_val_codebleu = val_codebleu\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(\"Best model saved.\")\n",
        "\n",
        "# Plotting loss and CodeBLEU scores\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_codebleu_scores, label=\"Validation CodeBLEU\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"CodeBLEU\")\n",
        "plt.title(\"CodeBLEU Scores\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6cCVhsmJxxjH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "53a6bbd7-d3b3-4118-a3c5-8f1c40714126"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1:   1%|          | 18/2000 [00:19<36:11,  1.10s/it, Average Loss=7.06, Train CodeBLEU=0]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-38d4ca94c228>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         outputs = model(input_ids=input_ids,\n\u001b[0m\u001b[1;32m     15\u001b[0m                         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                         \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/git/modeling_git.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, pixel_values, head_mask, inputs_embeds, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1480\u001b[0m             \u001b[0muse_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         outputs = self.git(\n\u001b[0m\u001b[1;32m   1483\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/git/modeling_git.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, pixel_values, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m                 \u001b[0;31m# here we assume pixel_values is of shape (batch_size, num_channels, height, width)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m                 \u001b[0mvisual_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/git/modeling_git.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         return self.vision_model(\n\u001b[0m\u001b[1;32m   1014\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/git/modeling_git.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    937\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify pixel_values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_layrnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/git/modeling_git.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0mtarget_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m         \u001b[0mpatch_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape = [*, width, grid, grid]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m         \u001b[0mpatch_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mPDdS_-wEoE3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "Now that we've trained the model, let's load the Maradona image and perform inference on it."
      ],
      "metadata": {
        "id": "mNZQXZrERyQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load image\n",
        "example = dataset_test[5]\n",
        "image = example[\"image\"]\n",
        "width, height = image.size\n",
        "display(image.resize((int(0.3*width), int(0.3*height))))"
      ],
      "metadata": {
        "id": "uC-Fp480XAt7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "a3008130-703a-4277-9a70-3f33cd091ad5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=240x240>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Batch size\n",
        "batch_size = 1\n",
        "# List to store the generated captions\n",
        "codes = []\n",
        "\n",
        "# Function to process a batch of images\n",
        "def process_batch(batch_images, prompt):\n",
        "    inputs = processor(images=batch_images, text=[prompt] * len(batch_images), return_tensors=\"pt\", padding=True).to('cuda')\n",
        "    pixel_values = inputs.pixel_values\n",
        "    generated_ids = model.generate(input_ids=inputs.input_ids, pixel_values=pixel_values, max_length=500)\n",
        "    generated_codes = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "    return generated_codes\n",
        "\n",
        "#Load best model:\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "model.eval()\n",
        "# Process dataset in batches\n",
        "for i in range(0, len(dataset_test), batch_size):\n",
        "    batch_images = [dataset_test[j]['image'] for j in range(i, min(i + batch_size, len(dataset_test)))]\n",
        "    batch_codes = process_batch(batch_images, prompt)\n",
        "    codes.extend(batch_codes)\n",
        "    print(f\"Processed {i} to {i + len(batch_images)}\")\n",
        "# Create a DataFrame from the codes list\n",
        "df = pd.DataFrame(codes, columns=[\"Generated_Code\"])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "\n",
        "# Optionally, you can save the DataFrame to a CSV file\n",
        "df.to_csv(\"generated_codes.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "6P3-u0eRxmsa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "bd7a7da5-e92b-4f32-b341-8c9601c1c862"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-ea5de29b95e1>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mbatch_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mbatch_codes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_codes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processed {i} to {i + len(batch_images)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-ea5de29b95e1>\u001b[0m in \u001b[0;36mprocess_batch\u001b[0;34m(batch_images, prompt)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mgenerated_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mgenerated_codes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgenerated_codes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m             \u001b[0;31m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   1825\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2456\u001b[0m         \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_initial_cache_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2458\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_unfinished_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2459\u001b[0m             \u001b[0;31m# prepare model inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2460\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device)\u001b[0m\n\u001b[1;32m   1986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mthis_peer_finished_flag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1988\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1989\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! We've successfully fine-tuned GIT on our tiny (image, text) dataset to generate captions of images of football players."
      ],
      "metadata": {
        "id": "JPPBnXsrP54j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "codes[0]"
      ],
      "metadata": {
        "id": "J25HDJnCR7ue"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}